% This file is iccc.tex.  It contains the formatting instructions for and acts as a template for submissions to ICCC.  It borrows liberally from the AAAI and IJCAI formats and instructions.  It uses the files iccc.sty, iccc.bst and iccc.bib, the first two of which also borrow liberally from the same sources.


\documentclass[letterpaper]{article}
\usepackage{iccc}


\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\pdfinfo{
/Title (Catchy Title)
/Subject (Proceedings of ICCC)
/Author (Paul Bodily)}
% The file iccc.sty is the style file for ICCC proceedings.
%
\title{Pop*}
\author{Paul Bodily and Dan Ventura\\
Computer Science Department\\
Brigham Young University\\
Provo, UT 84602  USA\\
ventura@cs.byu.edu\\
}
\setcounter{secnumdepth}{0}

\begin{document} 
\maketitle
\begin{abstract}
\begin{quote}
Abstract
\end{quote}
\end{abstract}

\section{Introduction}

Children possess the amazing ability to use concepts they already know to understand and even create new concepts. Models of this \textit{human-level concept learning} have proven to be extremely effective (better even than deep-learning algorithms) in one-shot classification, parsing, and generation of hand-written characters \cite{lake2015human}.

Concept learning works by factoring the joint distribution over hand-written character types into a product of conditional distributions,

\[ P(\psi) = P(\kappa) \prod_{i=1}^{\kappa} P(n_i|\kappa)P(S_i|i,n_i)P(R_i|S_1, ..., S_{i-1}), \]

\noindent where each conditional distribution is a model of a \textit{subconcept}: \( P(\kappa) \) models the number of strokes per character; \( P(n_i|\kappa) \) models the \# of substrokes for the $i$th stroke for a character with $\kappa$ strokes; \( P(S_i|i,n_i) \) models the $i$th stroke with $n_i$ substrokes; and \( P(R_i|S_1, ..., S_{i-1}) \) models the relation of the $i$th stroke to the previous strokes. Each of these models can be further decomposed. This process of decomposition allows the system to effectively learn subconcepts from empirical data in order to easily learn and generate new character types.

In this paper we investigate the strengths and weaknesses of human-level concept learning as a tool for creating computationally creative systems. In particular we find that concept learning provides a powerful framework for producing novel, typical, artefacts that invariably include elements of surprise by virtue of its wide range of expression.

We focus on the domain of lyrical pop music composition; however the principles are readily applicable in other domains. Lyrical pop music is an ideal subject insofar as it naturally decomposes into multiple sub-problems, each of which can then be further factored. The system we describe, which we call \textit{Pop*}, demonstrates how existing computational creative solutions can be readily incorporated in defining subconcept distributions, using the specific example of Pachet's constrained Markov model.

\section{Related Works}

Despite its worldwide popularity for generations, lyrical pop music generation has received limited attention from the computational research community. Magenta, JukeBox, ...

THIS DISCUSSION BELONGS SOMEWHERE, PERHAPS EXPANDED IN ITS OWN PAPER. We pause briefly to consider and address a few of the reasons for which pop music may have yet to garner significant interest within the research community. First, there is still a strong stigma against pop music as being inherently less musically sophisticated and therefore less musicologically valuable to humanity. Second, the term pop music (deriving from popular music) describes an eclectic variety of subgenres, depriving the term of any easily definable characteristics. Lastly, much of what lies within the domain of pop music remains highly proprietary, making it difficult for researchers to obtain access to sufficient data to train machine learning models.

To dismiss what is currently popular as being less sophisticated or less valuable than the stuff of yore is a cycle which has repeated itself as far back as Mozart and as recently as with Jazz music. We stand to gain new insights into human and computer creativity as we acknowledge and overcome our biases towards certain creative expressions (especially those with as much cultural and psychological influence as pop music).

Secondly, the vagary inherent in the definition of pop music provides an essential challenge to machine learning algorithms, which traditionally target and cater to problems with well-defined boundaries and examples. Models that generalize well in vast domains with relatively few training examples are essential to truly creative computational systems. As pop music has been popularly used to include genres as diverse as rap, indie, big band, broadway, and rock 'n' roll, the attempt to characterize models of popular music forces us to consider more generalized models of creativity, which have increased potential of having more cross-domain application and of discovering yet undiscovered subgenres of creativity (e.g., new pop music subgenres).

Finally, the challenge of accessing high-quality pop music datasets (whether acoustic or symbolic) is significant. There is a dearth of well-annotated resources for those interested in studying any or all of the aspects of pop music composition. Besides being highly proprietary, artefacts in music generally require relatively complex representations and relatively few possess the domain knowledge required to generate or transcribe the needed data. There is, however, much we can do to improve the situation that lies within our power. First, we need to make resources that \textit{are} available more accessible (guitar tabs, lyrics sites, beatles, wikifonia). Second, we need to establish a better case for how society and industries stand to benefit from computational pop music research in order to generate a productive dialogue for the support and collaboration of those in possession of large pop music datasets (sheet music sites, spotify, etc., asking for APIs, etc). Note that this is different than asking them to simply give us their proprietary data. Third, we need to recognize contributions of novel datasets. 

\section{Methods}

We define the joint probability distribution on inspirations $\iota$, compositions $\gamma$, and voicings $\phi^{(m)}$ as follows,

\[ P(\iota,\gamma,\phi^{(1)}, ..., \phi^{(m)}) = P(\iota)P(\gamma|\iota) \prod_{m=1}^{M} P(\phi^{(m)}|\iota,\gamma). \]

In essence we are decomposing the model of lyrical song composition to individually model the inspiration for the composition, the symbolic (abstract) representation of the composition, and the concrete rendering of the composition. Pop* is primarily designed as a definition of $P(\gamma|\iota)$, and we devote the bulk of this section to its description. We then briefly describe $P(\phi^{(m)}|\iota,\gamma)$ and how it is defined in our system. $P(\iota)$ is the focus of ongoing research and is discussed in the Discussion section.

One of the challenges of using the HBPL model is deciding how and how far to factor the joint distribution. Bayes' theorem suggests that the factoring is irrelevant: any factoring should reproduce the joint when the terms are multiplied:

\[ P(A,B) = P(A|B)P(B) = P(B|A)P(A). \]

However, in practice we quickly find the need to approximate distributions. Furthermore we at times make independence assumptions to increase the power of our models (as discussed below). The factorization will therefore only leave negligible ``fingerprints'' on the artefacts it can produce to the extent that each of the factors is accurately modeled.

 In music, this is somewhat akin to asking a composer, ``do you compose the melody first? Or the lyrics first?" Responses will vary; but it is (theoretically) possible that a given composition could be composed via differing methodologies.

Given that the space of possible of songs is essentially infinite, it can be challenging to accurately model the distribution for each subconcept with the relatively few songs that have actually been written. But often an approximation is sufficient to get a reasonable, working model. This use of an approximate distribution motivated implementing a modular framework for a few reasons. First, it affords the metacreator the opportunity to improve upon or substitute an alternative approximation function for any of the subcomponents. Second, multiple approximations can be combined to create an approximation function with a wider range of outputs.

\subsubsection{Data}

We trained submodels using lyrical pop composition data from the Wikifonia dataset which contains 6,673 compositions in MusicXML format. Depending on the submodel, the dataset was appropriately filtered (e.g., harmony model only trained on compositions with harmonies). All songs were normalized to the same starting key.

\subsection{Composition, $P(\gamma|\iota)$}

The conditional distribution on compositions $\gamma$, given an inspiration $\iota$, is defined as follows,

\[ P(\gamma|\iota) = P(\nu|\iota)P(\tau|\nu)P(\eta|\nu,\tau)P(\mu|\nu,\tau,\eta)P(\lambda|\nu,\tau,\mu), \] 

\noindent with the following variable definitions:

\(P(\nu|\iota):=\) prior over intentions $\nu$ given $\iota$,

\(P(\tau|\nu):=\) prior over structure $\tau$ given $\nu$,

\(P(\eta|\nu,\tau):=\) prior over harmony $\eta$ given $\nu$ and $\tau$,

\(P(\mu|\nu,\tau,\eta):=\) prior over melody $\mu$ given $\nu$, $\tau$, and $\eta$, and

\(P(\lambda|\nu,\tau,\mu):=\) prior over lyrics $\lambda$ given $\nu$, $\tau$, and $\mu$.

We further factor our model of structure $\tau$ as

\[ P(\tau|\nu) = P(\zeta|\nu)P(\sigma|\nu,\zeta) \]

\noindent where 

\(P(\zeta|\nu):=\) prior over global structure $\zeta$ given $\nu$ and

\(P(\sigma|\nu,\zeta):=\) prior over segment structure $\sigma$ given $\nu$ and $\zeta$.

This factorization of the prior on compositions makes several independence assumptions which will be discussed in the subsections that follow. Although this factorization is highly dependent on the domain of lyrical composition, there are strong cross-domain parallels for many of the factors which we will also examine.

\subsubsection{Intention, $P(\nu|\iota)$}

In Pop* we use \cite{bay:inpress-a}'s notion of intention to define the thematic, stylistic, and cultural objectives that should influence the composition.

\subsubsection{Global Structure, $P(\zeta|\nu)$}

NEED SOME MATH HERE

Many creative artefacts have some form of global structure, defining the boundary and relationships between subparts of an artefact. An example in literature might be the structure of the story line (e.g., "hero cycle"). In lyrical pop music, these subparts are readily apparent in the sequence of verses (V) and choruses (C) (defining large-scale repetitions in one or more musical viewpoints) and intros (I), outros (O), and bridges (B) (generally not wholly repeated). We refer to these subparts in our model as \textit{segments} and its value (e.g., "verse") as its \textit{segment type}.

There are several ways to approximate a distribution for global structure. We first implemented a severely limited approximation: a \textbf{fixed} structure (e.g., I,V,C,V,C,B,C,O). Despite the range of possible compositions that are uncomputable by this approximation, this limitation would likely be overlooked if enough variation exists in other subcomponent models.

We improved upon this approximation with a \textbf{distributional model} which creates a probability density function of possible structures as learned from a database of composition artefacts (see Figure of pdf!!!). While making possible a wider range of plausible structures, distributional models learned from data carry significant AI challenges. Consider, for example, that most compositions do not label the segments, and therefore they must somehow be inferred.

To infer segments we used a Needleman-Wunsch alignment over multiple viewpoints to find regions of an input composition where harmony, melody, and lyrics all matched (i.e., chorus) and where only harmony and melody matched (i.e., verse). All other segments (demarcated by the inferred verse/chorus segments) were labeled as intro, outro, interlude, or bridge according to their position in the composition and whether or not they contained lyrics.

\subsubsection{Segment Structure, $P(\sigma|\nu,\zeta)$}

The subparts defined by global structure often represent subconcepts that require further factorization or definition. For example, segments in a composition exhibit \textit{segment structure} in the number of measures; the number of syllables or notes; which lyrics rhyme; and how harmonic progressions or melodic sequences repeat. 

Besides implementing a \textbf{fixed} structure, we also tried learning a probability distribution of structure from existing compositions. 

Counting syllables, counting variation in syllable count over segments of the same type

Counting measures

Aligning melody

Aligning harmony

Aligning normalized melody

Aligning phonemes

\subsubsection{Harmony, $P(\eta|\nu,\tau)$}

Having sampled an abstract representation of the song, the system proceeds to construct the physical representation of the artefact, conditioned on the constraints imposed by the global or subglobal structure. In lyrical pop music composition, we summarize the physical representation in terms of harmony, melody, and lyrics, generated in that order. Each is conditioned on the structure and elements that have been previously sampled.

Our most basic implementation of an approximation for a model of harmonic sequences is the \textbf{monochord generator} which generates a single chord lasting the entire composition. Though a few monochord pop songs have been successful, the vast majority depend on more complex harmonic progressions. 

Though many grammar-based approaches exist for generating harmonic progression (e.g., \cite{steedman1984generative}), we opted to see how much could be learned using a strictly data-driven model. The \textit{distributional harmony} model is further factored into two independent submodels: the \textbf{distributional harmony duration} model and the \textit{distributional harmony chord} model

The decision to assume that duration and chord are independent, though potentially erroneous, was deliberate. This was based on the reasoning that the strength of a probabilistic model depends on the number of instances used to train the model. Each time a distribution adds a conditional variable, the strength of the model is drastically reduced (this also explains why I'm unimpressed when obscure sports records are broken and announced by sportscasters). We felt that the duration and chord are sufficiently independent to where the model strength recovered by assuming independence outweighed the cost of ignoring any relation between them. (Can we show this somehow?)

The distributional harmony duration and chord models were implemented as a constrained single-order Markov model \cite{pachet2001finite}. Each was additionally conditioned on time signature, measure position, and segment type. Our intuition was that conditioning on segment type would allow a chorus (which may perhaps have more powerful progressions or stronger resolutions) to distinguish itself harmonically from a bridge (which tends towards less resolution).

Markov models with constraints don't work for lengths that aren't in terms of tokens. How to deal with length? Perhaps have each token represent a specific duration?

\subsubsection{Melody, $P(\mu|\nu,\tau,\eta)$}



\subsubsection{Lyrics, $P(\lambda|\nu,\tau,\mu)$}

Several models of natural language generation and in particular NLG in poetry and music have been published. As these models continue to improve, so will their application in lyrical composition. This demonstrates the relative robustness of the HBPL framework: as improved models are conceived and implemented, the joint model is also improved.

To approximate a distribution for $P(\lambda|\nu,\tau,\mu)$ we developed an independent module called Lyrist \cite{bay:inpress-a}. Lyrist uses existing lyric segments as templates for the creation of novel lyric segments according to $\tau$. Templates are selected as a function of the length (in note count) of $\mu$. Lyrist intelligently chooses which words to replace by finding those words which... Given an original word $l$ uses the word embedding $W(l)$ to intelligently choose a replacement word $l'$. Lyrist uses the cultural intention of $\nu$ in selecting the training corpus for the word embedding model and the thematic intention of $\nu$ as a seed for finding a set of candidates for $l'$. These candidates are then filtered according to constraints imposed by the structural intention of $\nu$ and the morphosyntactic POS tag of $l$ \cite{bay:inpress-a}.

The advantage of using a template-based approach to lyrics generation is that it does well at maintaining syntactical coherence. The primary shortcomings are that resulting lyrics provide minimal syntactic novelty from the training data and make no inherent effort at providing global semantic cohesion.

Other things we hope to try include...

  (ALGORITHM POPOUT BOX)

\subsection{Voicing, $P(\phi^{(m)}|\iota,\gamma)$}

The goal of Pop* was to develop a model of lyrical compositions $\gamma$ (i.e., a leadsheet). However, evaluating such an artifact inevitably requires a concrete rendering of the artefact, whose distribution can we have modeled according to the term $P(\phi^{(m)}|\iota,\gamma)$. We chose a simple voicing module which comps piano chords containing notes defined by the harmony in a middle keyboard range together with a comping bass line (also determined by the harmony). The key for the voiced composition was sampled from the empirical frequency of keys in the training set. 

The artefact is rendered both as leadsheet and a full score (i.e., with piano and bass notes played) in MusicXML format. An MP3 audio featuring computer-sung lyrics recording is then produced using Harmony Assistant (v9.7.0f) and Virtual Singer (v3.2).

\section{Results}

\section{Discussion}

There is much that can be drawn from the parallel between concept learning in children and concept learning in computers. One common criticism of probabilistic models is that they require too much hand-holding and manual construction. While the criticism is accurate?defining the submodels requires both domain knowledge and is time-intensive?it is a mistake to assume that any system possessing these attributes is inherently worse than a system requiring no domain knowledge or customized implementation. As a simple example, one need only consider how much ``hand-holding'' is required to teach a child to read. The Direct Instruction System for Teaching Arithmetic and Reading (DISTAR) method, which uses a rule-learning approach very similar to the human-level concept learning we've described, has been touted as one of the most effective for teaching children to read and do math. There are a lot of parallels here: metacreator = parent (both prone to biases), concepts have to be taught one-by-one (not sufficient to just give the kid a bunch of books and say ``practice reading''), kid eventually learns this skill.

\subsection{Inspiration}

In the joint probability distribution on inspirations $\iota$, compositions $\gamma$, and voicings $\phi^{(m)}$, we define $P(\iota)$ not as the prior on intentions, but as the prior on the inspiring source for the intention. In other words, not ``what was the artefact intended to communicate?'', but ``what was the inspiring source for what the artefact intended to communicate?''

This represents an addition to the model originally presented by \cite{lake2015human}: not only are we modeling \textit{which} artefacts can be generated, but also \textit{why} they are generated. 

There is some benefit to the vagary of factorization: we can condition on any variable that could be argued to influence the song's composition. Despite the a long-standing tradition in CC that (inspiration) is essential to creativity, there have been many arguably-creative systems described elsewhere in the literature where inspiration effectively gets ``lost in the wash''. With the concept learning framework, we can model this attribute explicitly.

In our system we distinguish between inspiration and intention. \textit{Inspiration} represents an environmental impetus that serves to shape the intention (e.g., ``my horse died''). \textit{Intention} (discussed below) represents what/how we want to communicate thematically, structurally, or culturally (e.g., ``sadness'' and ``country vernacular'').

Many creative systems do not explicitly model inspiration, but rather use a model that randomly selects an intention or which defers to the user to select an intention. Here may be where a great deal more effort could be devoted to defining the ``creative spark'' that determines an artefact's intention. For example, observers often perceive greater creativity in artefacts which in some way relate to them or to their culture \cite{colton2008creativity}. Therefore using an observer's environment or culture as an inspiring source is one possible way to model inspiration. Recent research in Electroencephalogram (EEG)-based affective computing (i.e., reading brain waves) suggests that computers may soon be endowed with perceptive capabilities of an observers emotional state beyond those of their human counterparts \cite{volioti2016mapping}.

\subsection{Big (Need for) Data}

As mentioned above, each of the empirically-driven models requires training on a dataset representative of the artefact domain. Even if we had digital access to all of the compositions ever written, it would represent an infinitesimal portion of the songs that could be written. This is a challenge in many machine learning domains. Unique to the pop music domain, however, is that data is highly proprietary. What \textit{is} available is extremely limited and of relatively poor quality. Compared to natural language, the language of music is notationally more complex and less commonly used or understood. Among those who \textit{do} understand and use it, music formatting can vary wildly and inexactly---creating additional challenges for a by-the-bit computer parser. Computers will only learn to speak music as quickly as we either formalize and ubiquitize the language of music \textit{or} endow computers with AI tools (e.g., ears to listen) to fill in the gaps on their own.

\section{Conclusion}

\bibliographystyle{iccc}
\bibliography{iccc}


\end{document}
